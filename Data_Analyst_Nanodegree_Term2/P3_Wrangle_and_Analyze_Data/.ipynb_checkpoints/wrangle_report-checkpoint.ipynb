{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "To have all [WeRateDogs](https://en.wikipedia.org/wiki/WeRateDogs) tweets data gathered and cleaned for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rundown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole wrangling process consists of three sections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Gathering\n",
    "2. Data Assessing\n",
    "3. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes the steps to gather the data needed for the analysis. The goal is to have the data ready for data assessing afterwards. Three datasets need to be gathered:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. WeRateDogs Twitter archive - Downloaded directly from the classroom as instructed\n",
    "2. Tweet image predictions - Downloaded programmatically with the help of [Requests](http://docs.python-requests.org/en/master/)\n",
    "3. Tweet JSON - Downloaded programmatically with the help of [Tweepy](http://www.tweepy.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To caveat, querying all of the tweet IDs in the WeRateDogs Twitter archive would take about 20-30 minutes, and Twitter does have a [rate limit](https://developer.twitter.com/en/docs/basics/rate-limiting). Therefore, a few tunes on the parameters, `wait_on_rate_limit` and `wait_on_rate_limit_notify` in `tweepy.api` [class](http://docs.tweepy.org/en/v3.2.0/api.html#API) would be nice. Also, a progress tracker like [tqdm](https://pypi.python.org/pypi/tqdm) would come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Assessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes the steps to assess data. The goal is to pinpoint and summarize the issues (Both quality and tidiness) that need to be tackled during data cleaning afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Quality Issues***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Archive Enhanced data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 181 observations of retweets are found\n",
    "2. 59 observations of missing expanded url (NaN)\n",
    "3. Inapproproiate datatypes for timestamp and dog stages\n",
    "4. Erroneous dog names (a, an, the etc.)\n",
    "5. Inappropriate string representation of missing dog names ('None')\n",
    "6. Redundant column (Denominator), given the fact that the rating is on a out-of-10 scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Predictions data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inappropriate datatype for dog breed (all p columns)\n",
    "2. Letters inconsistency in which words sometimes start with lower case letter/ upper case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet JSON data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 172 observations of retweets are found\n",
    "2. Unclear/ confusing column naming (eg. id)\n",
    "3. Redundant columns are included (May depend on what sort of analyses need to be performed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tidiness Issues***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Archive Enhanced data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dog stages as a single variable (Categorical) are represented by four separate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Predictions data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Image URLs, predictions, and results should be part of the archive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweet JSON data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Retweet and favorite count should be part of the archive data\n",
    "2. Text column duplicated in the archive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is not a super huge dataset, there are numbers of variables. It is highly recommended to assess the datasets programmatically with pandas functions like: \n",
    "1. [info()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n",
    "2. [head()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html)\n",
    "3. [tail()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html)\n",
    "4. [sample()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes the steps to clean the data. The goal is to create one well consolidated dataset for analysis afterwards. Issues can most be tackled by the pandas library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Observations of retweets - Subset datasets to exclude unwanted observations\n",
    "2. Missing expanded urls - Subset datasets to exclude unwanted observations\n",
    "3. Inappropriate data types - pandas [astype()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html) and [to_datetime()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html) function\n",
    "4. Erroneous dog names and inappropriate representations - pandas [rename()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html) function\n",
    "5. Letters inconsistency - pandas [str.lower()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.lower.html) function\n",
    "6. Confusing column name - pandas rename() function\n",
    "7. Redundant columns - pandas [drop()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the quality issues are tackled, the three separate datasets can be merged by pandas [merge()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) function. Some final refinements may be needed for the tidiness, but pandas functions like the ones above will mostly suffice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
